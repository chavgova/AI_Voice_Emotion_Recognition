- using only mel (128 values) -> 27% acc /250 epochs 32 batch size 
- using 2 x Max pooling 16, 8-> 33% acc /250 epochs 32 batch size 
- using opt = Adam (lr = 0.0001) -> 63% acc /250 epochs 32 batch size; around 170 epoch - 63%; big overfit
- using AveragePooling1D (ps = 8) -> NOPE
- using MaxPooling(5) + Adam -> 73% acc around 100-130 epoch; big overfit  ****
- using activation tanh (last one - relu) -> 52% max
- using all sigmoid activations -> NOPE
- using all selu activations -> 65% acc
- using MaxPooling(3) + MaxPooling(2) + selu -> 60% acc
- using sulu + relu -> 63% (250 epochs); 
- using gelu + relu -> 40% (70 epochs); all gelu -> 40% (90 epochs)
- using relu + leaky relu -> max 65% acc
- using opt = Adadelta -> not moving from 5%
- using relu opt = Nadam -> ~70% acc (130 epochs); big overfit   ****
- using opt = Adagrad -> NOPE
- using opt = Adamax -> 67% acc; overfit ****
- using different numbers of neurons (256,256,128..); relu; NO MaxPooling; 5,7,5,3,1,5; Adam -> 68% acc
- NOT using MaxPooling; relu -> ~70%; overfit    ****
- NOT using MaxPooling + different numbers of neurons + relu; Adamax ->  67% acc; overfit 
- NOT using MaxPooling + relu; Adam; 3,3,3,3,3, MaxP,3; -> 65% acc; big overfit
- NOT using MaxPooling + relu; Adam; 3,3,3,3,3,3; -> 58% acc; big overfit
- using MaxPooling(4) + relu; Adam; 7,7,7,7, MP,7,7; -> 68% acc; big overfit 
- NEW dataset - TESSS (relu, Adam) - 89% acc     *****
- without genders ->
- NEW dataset - SAVEE (relu, Adam) - 87% acc
- 2 CNN - 1 for each gender ->


- tanh
- filters- 225,128,128,64,64-d,64,32-d,8-d
- kernel_regularizer=keras.regularizers.l2(0.001) -> less overfitting
- kernel size - 20,15,10,5 -> loss: 0.4769 - accuracy: 0.9908 - val_loss: 0.8212 - val_accuracy: 0.8655
- kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4) -> 89-90% acc
- bias_regularizer=regularizers.l2(1e-4) -> loss: 0.2668 - accuracy: 0.9980 - val_loss: 0.6475 - val_accuracy: 0.8495
- activity_regularizer=regularizers.l2(1e-5) -> loss: 0.3786 - accuracy: 0.9974 - val_loss: 0.7919 - val_accuracy: 0.8410
- kernel size - 1,2,3,4,5 -> loss: 0.2643 - accuracy: 0.9969 - val_loss: 0.6908 - val_accuracy: 0.8357
- kernel size - 3,6,10,6,3 -> loss: 0.2890 - accuracy: 0.9964 - val_loss: 0.6524 - val_accuracy: 0.8491


